{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无监督学习与聚类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN，决策树，随机森林都是比较常用的机器学习算法，他们虽然有着不同的功能，但却都属于“有监督学习”的一部分，即是说，模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当中，还有相当一部分算法属于“无监督学习”，无监督的算法在训练的时候只需要特征矩阵X，不需要标签。无监督学习的代表算法有聚类算法、降维算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 聚类：  \n",
    "聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组（或簇）。这种划分可以基于我们的业务需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。\n",
    "\n",
    ">比如在商业中，如果我们手头有大量的当前和潜在客户的信息，我们可以使用聚类将客户划分为若干组，以便进一步分析和开展营销活动，最有名的客户价值判断模型RFM（Recency Frequency Monetary），就常常和聚类分析共同使用。\n",
    "\n",
    ">再比如，聚类可以用于降维和矢量量化（vector quantization），可以将高维特征压缩到一列当中，常常用于图像，声音，视频等非结构化数据，可以大幅度压缩数据量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](image\\聚类与分类.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans的基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans是如何工作的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为聚类算法的典型代表，KMeans可以说是最简单的聚类算法没有之一，那它是怎么完成聚类的呢？\n",
    "![title](image\\簇与质心.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在KMeans算法中，簇的个数K是一个超参数，需要我们人为输入来确定。KMeans的核心任务就是根据我们设定好的K，找出K个最优的质心，并将离这些质心最近的数据分别分配到这些质心代表的簇中去。具体过程可以总结如下：\n",
    "![title](image\\均值聚类流程.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那什么情况下，质心的位置会不再变化呢？当我们找到一个质心，在每次迭代中被分配到这个质心上的样本都是一致的，即每次新生成的簇都是一致的，所有的样本点都不会再从一个簇转移到另一个簇，质心就不会变化了。  \n",
    "这个过程在可以由下图来显示，我们规定，将数据分为4簇（K=4），其中白色X代表质心的位置：\n",
    "![title](image\\聚类迭代.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 簇内误差平方和的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类算法聚出的类有什么含义呢？这些类有什么样的性质？我们认为，**被分在同一个簇中的数据是有相似性的，而不同簇中的数据是不同的**，当聚类完毕之后，我们就要分别去研究每个簇中的样本都有什么样的性质，从而根据业务需求制定不同的商业或者科技策略。聚类算法的目的就是追求“簇内差异小，簇外差异大”。而这个“差异“，由**样本点到其所在簇的质心的距离**来衡量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个簇来说，所有样本点到质心的距离之和越小，我们就认为这个簇中的样本越相似，簇内差异就越小。而距离的衡量方法有多种，令$x$表示簇中的一个样本点，$\\mu$表示该簇中的质心，$n$表示每个样本点中的特征数目，$i$表示组成点$x$的每个特征，则该样本点到质心的距离可以由以下距离来度量：\n",
    "![title](image\\距离.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，m为一个簇中样本的个数，j是每个样本的编号。这个公式被称为**簇内平方和**（cluster Sum of Square），又叫做Inertia。而将一个数据集中的所有簇的簇内平方和相加，就得到了整体平方和（Total Cluster Sum of Square），又叫做total inertia。Total Inertia越小，代表着每个簇内样本越相似，聚类的效果就越好。**因此KMeans追求的是，求解能够让Inertia最小化的质心**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了欧几里得距离，还可以使用其他距离，在Kmeans中，只要使用了正确的质心和距离，都可以达到不错的效果：\n",
    "![title](image\\距离度量.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用sklearn实现KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.cluster.KMeans (n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, tol=0.0001,precompute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=None,algorithm=’auto’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数\n",
    ">n_clusters：数据集分为几簇(类)，默认为8，唯一必填的参数   \n",
    ">random_state：控制每次质心随机初始化的随机数种子\n",
    "\n",
    ">init：可输入\"k-means++\"，\"random\"或者一个n维数组，默认\"k-means++\"\n",
    ">>输入\"k-means++\"：一种为K均值聚类选择初始聚类中心的聪明的办法，以加速收敛  \n",
    ">>如果输入了n维数组：数组的形状应该是(n_clusters，n_features)并给出初始质心  \n",
    "\n",
    ">n_init：默认10，可以使用参数n_init来选择，每个随机数种子下运行的次数。\n",
    ">>如果不指定随机数种子，则sklearn中的K-means并不会只选择一个随机模式扔出结果，而会在每个随机数种子下运行多次，并使用结果最好的一个随机数种子来作为初始质心。\n",
    "\n",
    ">max_iter：整数，默认300单次运行kmeans允许的最大迭代次数\n",
    "\n",
    ">tol：浮点数，默认1e-4两次迭代间Inertia下降的量，如果两次迭代之间Inertia下降的值小于tol所设定的值，迭代就会停下\n",
    "\n",
    ">n_iter_：迭代次数\n",
    "\n",
    "属性\n",
    ">labels_：查看聚好的类别，每个样本所对应的类  \n",
    ">cluster_centers_：查看质心  \n",
    ">inertia_：查看总距离平方和  \n",
    "\n",
    "接口\n",
    ">fit_predict：所得到的结果和我们不调用predict，直接fit之后调用属性labels一模一样  \n",
    ">predict：数据量非常大的时候，可以使用部分数据来确认质心，剩下数据的聚类结果，使用predict来调用  \n",
    "这样的结果，肯定与直接fit全部数据会不一致。有时候，当我们不要求那么精确，或者我们的数据量实在太大，那我们可以使用这样的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚类算法的模型评估指标：轮廓系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertia(总距离平方和)是用距离来衡量簇内差异的指标，但是这个指标的缺点和极限太大：\n",
    "- 首先，它不是有界的。只知道，Inertia是越小越好，是0最好，但我们不知道，一个较小的Inertia究竟有没有达到模型的极限，能否继续提高。  \n",
    "- 第二，它的计算太容易受到特征数目的影响，数据维度很大的时候，Inertia的计算量会陷入维度诅咒之中，计算量会爆炸，不适合用来一次次评估模型。  \n",
    "- 第三，它会受到超参数K的影响，在我们之前的尝试中其实我们已经发现，随着K越大，Inertia注定会越来越小，但这并不代表模型的效果越来越好了。  \n",
    "- 第四，Inertia对数据的分布有假设，它假设数据满足凸分布，并且它假设数据是各向同性的，即是说数据的属性在不同方向上代表着相同的含义。\n",
    "\n",
    "下图表示了Inertia的缺陷：  \n",
    "![title](image\\Inertia缺陷.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans的目标是确保“簇内差异小，簇外差异大”,因此轮廓系数是最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：\n",
    "- 1. 样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离\n",
    "- 2. 样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中的所有点之间的平均距离\n",
    "\n",
    "根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。\n",
    "单个样本的轮廓系数计算为：\n",
    "![title](image\\轮廓系数公式.png)  \n",
    "很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一个簇中的大多数样本具有比较高的轮廓系数，则簇会有较高的总轮廓系数，则整个数据集的平均轮廓系数越高，则聚类是合适的。如果许多样本点具有低轮廓系数甚至负值，则聚类是不合适的，聚类的超参数K可能设定得太大或者太小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn中计算轮廓系数\n",
    "- 1. 使用模块metrics中的类silhouette_score来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。\n",
    "- 2. 还有同在metrics模块中的silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "轮廓系数有很多优点，它在有限空间中取值，使得我们对模型的聚类效果有一个“参考”。并且，轮廓系数对数据的分布没有假设，因此在很多数据集上都表现良好。但它在每个簇的分割比较清洗时表现最好。但轮廓系数也有缺陷，它在凸型的类上表现会虚高，比如基于密度进行的聚类，或通过DBSCAN获得的聚类结果，如果使用轮廓系数来衡量，则会表现出比真实聚类效果更高的分数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例：基于轮廓系数来选择n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例：聚类算法用于降维，KMeans的矢量量化应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means聚类最重要的应用之一是非结构数据（图像，声音）上的矢量量化（VQ）。非结构化数据往往占用比较多的储存空间，文件本身也会比较大，运算非常缓慢，我们希望能够在保证数据质量的前提下，尽量地缩小非结构化数据的大小，或者简化非结构化数据的结构。矢量量化就可以帮助我们实现\n",
    "这个目的。KMeans聚类的矢量量化本质是一种降维运用，但它与我们之前学过的任何一种降维算法的思路都不相同。特征选择的降维是直接选取对模型贡献最大的特征，PCA的降维是聚合信息，而**矢量量化的降维是在同等样本量上压缩信息的大小**，即不改变特征的数目也不改变样本的数目，只改变在这些特征下的样本上的信息量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "浮点数运算比整数快"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
